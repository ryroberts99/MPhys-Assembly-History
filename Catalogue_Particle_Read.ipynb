{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0802f2cc",
   "metadata": {},
   "source": [
    "# Catalogue and Particle Reading Scripts\n",
    "### This notebook contains the scripts for obtaining both catalogue data and particle data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdbb033",
   "metadata": {},
   "source": [
    "## Catalogue Read Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6204d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54cc92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyread_eagle\n",
    "import matplotlib.pyplot as plt\n",
    "from pyread_eagle import EagleSnapshot\n",
    "import os\n",
    "import h5py as h5\n",
    "\n",
    "h = 0.6727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogue_read(table,\n",
    "                   quantity,\n",
    "                   data_location,\n",
    "                   sim = 'Organic',\n",
    "                   model = 'RECAL',\n",
    "                   tag = '028_z000p000',\n",
    "                   phys_units=True,\n",
    "                   cgs_units=False,                     \n",
    "                   verbose=False):\n",
    "\n",
    "    # Do a quick check to make sure a valid table has been specified\n",
    "    assert table in ['FOF','Subhalo'],'table must be either FOF or Subhalo'\n",
    "\n",
    "    subfindfile_pattern = os.path.join(data_location , model , sim , f'groups_{tag}', f'eagle_subfind_tab_{tag}.{{}}.hdf5')\n",
    "    \n",
    "    file_ind = 0\n",
    "    data_arr = None\n",
    "    dt = None\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            with h5.File(subfindfile_pattern.format(file_ind), 'r') as f:\n",
    "                try:\n",
    "                    \n",
    "                    data = f['/'+table+'/'+quantity]\n",
    "\n",
    "                    if file_ind == 0:\n",
    "\n",
    "                        h_scale_exponent = data.attrs['h-scale-exponent']\n",
    "                        a_scale_exponent = data.attrs['aexp-scale-exponent']\n",
    "                        cgs_conversion_factor = data.attrs['CGSConversionFactor']\n",
    "                        h = f['Header'].attrs['HubbleParam']\n",
    "                        a = f['Header'].attrs['ExpansionFactor']\n",
    "\n",
    "                        # Let's only print this stuff out if the user wants it!\n",
    "                        if verbose:\n",
    "                            print('Loading ',quantity)\n",
    "                            print('h exponent = ',h_scale_exponent)\n",
    "                            print('a exponent = ',a_scale_exponent)\n",
    "                            print('cgs conversion factor = ',cgs_conversion_factor)\n",
    "                            print('h = ',h)\n",
    "                            print('a = ',a)\n",
    "\n",
    "                        # Lets be a bit more cautious and make sure we cast our HDF5 dataset into an array of the correct type\n",
    "                        dt = data.dtype\n",
    "                        \n",
    "                        data_arr = np.array(data, dtype = dt)\n",
    "                    \n",
    "                    data_arr = np.append(data_arr,np.array(data,dtype=dt),axis=0)\n",
    "\n",
    "                        \n",
    "                except KeyError: \n",
    "                    pass\n",
    "                        \n",
    "        except OSError:\n",
    "            if verbose:\n",
    "                print('Run out of files after loading ',file_ind)\n",
    "            break\n",
    "\n",
    "        file_ind += 1\n",
    "    \n",
    "    # If the data we're loading is integer-type, no corrections will be needed\n",
    "    if np.issubdtype(dt,np.integer):\n",
    "        return data_arr\n",
    "\n",
    "    # Otherwise, do unit corrections\n",
    "    else:\n",
    "\n",
    "        if phys_units:\n",
    "            h = 0.6727\n",
    "            h_scale_exponent = -1\n",
    "            a = 1\n",
    "            a_scale_exponent = 1\n",
    "            data_arr *= np.power(h,h_scale_exponent) * np.power(a,a_scale_exponent)\n",
    "\n",
    "        if cgs_units:\n",
    "\n",
    "            # cgs numbers can be huge and overflow np.float32\n",
    "            # Recast the data to float64 to be safe\n",
    "            data_arr = np.array(data_arr,dtype=np.float64)\n",
    "\n",
    "            data_arr *= cgs_conversion_factor\n",
    "\n",
    "        return data_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2b9d7",
   "metadata": {},
   "source": [
    "## Particle Read Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd3ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_snapshot_template(data_location, model, sim, tag): \n",
    "    return os.path.join(data_location , model , sim , f'particledata_{tag}', f'eagle_subfind_particles_{tag}.{{}}.hdf5')\n",
    "    \n",
    "\n",
    "def particle_read(ptype,\n",
    "                  quantity,\n",
    "                  lower_bounds,\n",
    "                  upper_bounds,\n",
    "                  data_location,\n",
    "                  snapshot,\n",
    "                  centre=None,\n",
    "                  sim = 'Organic',\n",
    "                  model = 'RECAL',\n",
    "                  tag = '028_z000p000',\n",
    "                  phys_units=True,\n",
    "                  cgs_units=False,\n",
    "                  verbose=False):\n",
    "    \n",
    "    # The arguments are xmin,xmax,ymin,ymax,zmin,zmax\n",
    "    snapshot.select_region(lower_bounds[0], upper_bounds[0], lower_bounds[1], upper_bounds[1], lower_bounds[2], upper_bounds[2])\n",
    "\n",
    "\n",
    "    snapshot_pattern = make_snapshot_template(data_location , model , sim , tag)\n",
    "    data_arr = None\n",
    "    dt = None\n",
    "    # Initialise an index\n",
    "    snapfile_ind = 0\n",
    "    while True:\n",
    "        try:\n",
    "            with h5.File(snapshot_pattern.format(snapfile_ind), 'r') as f:\n",
    "                # Grab all the conversion factors from the header and dataset attributes\n",
    "                h = f['Header'].attrs['HubbleParam']\n",
    "                a = f['Header'].attrs['ExpansionFactor']\n",
    "                h_scale_exponent = f['/PartType%i/%s'%((ptype,quantity))].attrs['h-scale-exponent']\n",
    "                a_scale_exponent = f['/PartType%i/%s'%((ptype,quantity))].attrs['aexp-scale-exponent']\n",
    "                cgs_conversion_factor = f['/PartType%i/%s'%((ptype,quantity))].attrs['CGSConversionFactor']\n",
    "        except:\n",
    "            if verbose:\n",
    "                # If there are no particles of the right type in chunk 0, move to the next one\n",
    "                print('No particles of type ',ptype,' in snapfile ',snapfile_ind)\n",
    "            snapfile_ind += 1\n",
    "            continue\n",
    "        # If we got what we needed, break from the while loop\n",
    "        break\n",
    "\n",
    "    # Load in the quantity\n",
    "    data_arr = snapshot.read_dataset(ptype,quantity)\n",
    "\n",
    "    # Cast the data into a numpy array of the correct type\n",
    "    dt = data_arr.dtype\n",
    "    data_arr = np.array(data_arr,dtype=dt)\n",
    "    x = len(data_arr)\n",
    "    if data_arr.shape == (x,3):\n",
    "        \n",
    "        data_arr = data_arr[(data_arr[:,0] >= lower_bounds[0]) & (data_arr[:,0] <= upper_bounds[0]) & (data_arr[:,1] >= lower_bounds[1]) & (data_arr[:,1] <= upper_bounds[1]) & (data_arr[:,2] >= lower_bounds[2]) & (data_arr[:,2] <= upper_bounds[2])]\n",
    "\n",
    "        \n",
    "#    if centre is not None:\n",
    "#        #Handling the periodic box (optionally)\n",
    "#        data_arr = ((data_arr - centre + (boxsize/2)) % boxsize) - (boxsize/2)\n",
    "\n",
    "    if not np.issubdtype(dt,np.integer):\n",
    "        # Do unit corrections, like we did for the catalogues\n",
    "        if phys_units:\n",
    "            data_arr *= np.power(h,h_scale_exponent) * np.power(a,a_scale_exponent)\n",
    "\n",
    "        if cgs_units:\n",
    "            data_arr = np.array(data_arr,dtype=np.float64)\n",
    "            data_arr *= cgs_conversion_factor\n",
    "  \n",
    "        \n",
    "\n",
    "    return data_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ef6b2",
   "metadata": {},
   "source": [
    "def make_snapshot_template(data_location, model, sim, tag): \n",
    "    return os.path.join(data_location , model , sim , f'particledata_{tag}', f'eagle_subfind_particles_{tag}.{{}}.hdf5')\n",
    "    \n",
    "\n",
    "def particle_read_sphere(ptype,\n",
    "                  quantity,\n",
    "                  centre=None,\n",
    "                  radius,\n",
    "                  data_location,\n",
    "                  snapshot,\n",
    "                  sim = 'Organic',\n",
    "                  model = 'RECAL',\n",
    "                  tag = '028_z000p000',\n",
    "                  phys_units=True,\n",
    "                  cgs_units=False,\n",
    "                  verbose=False):\n",
    "    \n",
    "    # The arguments are xmin,xmax,ymin,ymax,zmin,zmax\n",
    "    snapshot.select_region(lower_bounds[0], upper_bounds[0], lower_bounds[1], upper_bounds[1], lower_bounds[2], upper_bounds[2])\n",
    "\n",
    "\n",
    "    snapshot_pattern = make_snapshot_template(data_location , model , sim , tag)\n",
    "    \n",
    "    # Initialise an index\n",
    "    snapfile_ind = 0\n",
    "    while True:\n",
    "        try:\n",
    "            with h5.File(snapshot_pattern.format(snapfile_ind), 'r') as f:\n",
    "                # Grab all the conversion factors from the header and dataset attributes\n",
    "                h = f['Header'].attrs['HubbleParam']\n",
    "                a = f['Header'].attrs['ExpansionFactor']\n",
    "                h_scale_exponent = f['/PartType%i/%s'%((ptype,quantity))].attrs['h-scale-exponent']\n",
    "                a_scale_exponent = f['/PartType%i/%s'%((ptype,quantity))].attrs['aexp-scale-exponent']\n",
    "                cgs_conversion_factor = f['/PartType%i/%s'%((ptype,quantity))].attrs['CGSConversionFactor']\n",
    "        except:\n",
    "            if verbose:\n",
    "                # If there are no particles of the right type in chunk 0, move to the next one\n",
    "                print('No particles of type ',ptype,' in snapfile ',snapfile_ind)\n",
    "            snapfile_ind += 1\n",
    "            continue\n",
    "        # If we got what we needed, break from the while loop\n",
    "        break\n",
    "\n",
    "    # Load in the quantity\n",
    "    data_arr = snapshot.read_dataset(ptype,quantity)\n",
    "\n",
    "    # Cast the data into a numpy array of the correct type\n",
    "    dt = data_arr.dtype\n",
    "    data_arr = np.array(data_arr,dtype=dt)\n",
    "    \n",
    "    data_arr = data_arr[(data_arr[:,0] >= lower_bounds[0]) & (data_arr[:,0] <= upper_bounds[0]) & (data_arr[:,1] >= lower_bounds[1]) & (data_arr[:,1] <= upper_bounds[1]) & (data_arr[:,2] >= lower_bounds[2]) & (data_arr[:,2] <= upper_bounds[2])]\n",
    "    \n",
    "#    if centre is not None:\n",
    "#        #Handling the periodic box (optionally)\n",
    "#        data_arr = ((data_arr - centre + (boxsize/2)) % boxsize) - (boxsize/2)\n",
    "\n",
    "    if not np.issubdtype(dt,np.integer):\n",
    "        # Do unit corrections, like we did for the catalogues\n",
    "        if phys_units:\n",
    "            data_arr *= np.power(h,h_scale_exponent) * np.power(a,a_scale_exponent)\n",
    "\n",
    "        if cgs_units:\n",
    "            data_arr = np.array(data_arr,dtype=np.float64)\n",
    "            data_arr *= cgs_conversion_factor\n",
    "  \n",
    "        \n",
    "\n",
    "    return data_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
